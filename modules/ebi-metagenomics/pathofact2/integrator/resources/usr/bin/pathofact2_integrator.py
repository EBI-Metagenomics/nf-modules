#!/usr/bin/env python

# Copyright 2024-2026 EMBL - European Bioinformatics Institute
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import argparse
import fileinput
import logging
import os

# Set up logger
logger = logging.getLogger(__name__)


def validate_inputs(
    to_validate_inputs: dict[str, str | None],
) -> dict[str, str | None]:
    """
    Validate that input files exist and are not empty or header-only.

    Args:
        to_validate_inputs: Dictionary mapping input names to file paths (can be None)

    Returns:
        Dictionary of non-valid input names and reason. If the number of keys is >0, the output is not generated
    """
    non_valid_inputs: dict[str, str | None] = {}

    for name, path in to_validate_inputs.items():
        if not path:
            non_valid_inputs[name] = "No input provided"
            continue

        # Check existence
        if not os.path.exists(path):
            logger.warning(f"File not found for '{name}' → {path}")
            non_valid_inputs[name] = f"File not found in {path}"
            continue

        # Read first few lines using hook_compressed (handles .gz automatically)
        with fileinput.hook_compressed(
            path, "r", encoding="utf-8", errors="ignore"
        ) as input_table:
            lines = [
                line for _, line in zip(range(3), input_table)
            ]  # read up to 3 lines

        # Check for empty files
        if not lines:
            logger.info(f"Skipping empty file for '{name}' → {path}")
            non_valid_inputs[name] = "Empty file"
            continue

        # Check for header-only file
        if len(lines) == 1:
            logger.info(f"Skipping header-only file for '{name}' → {path}")
            non_valid_inputs[name] = "Header only"
            continue

    return non_valid_inputs


def parse_pathofact_support(input_file: str) -> dict[str, str]:
    """
    Parse Pathofact2 prediction result in tsv format

    Args:
        input_file: Path to the tsv input file
                    This file has 5 columns:
                    sequence_id     detection_method        support_value_type      support_value   vfdb_hit
    Returns:
        pathofact_attributes defaultdict with protein id as key and list of annotations as values
    """
    pathofact_attributes: dict[str, str] = {}
    with fileinput.hook_compressed(input_file, "r", encoding="utf-8") as input_table:
        # Skipping the header line
        next(input_table)
        for line in input_table:
            line_l: list[str] = line.rstrip().split("\t")
            protein_id: str = line_l[0]
            method: str = line_l[1]
            support_value: str = line_l[3]

            if method == "blastp":
                vfdb_hit: str = line_l[4]
                annotation_line = ";".join(
                    [
                        ("vfdb=" + vfdb_hit),
                        ("blastp_eval=" + support_value),
                    ]
                )
            else:
                annotation_line = method + "_prob=" + support_value

            if protein_id in pathofact_attributes:
                stored_line = pathofact_attributes[protein_id]
                new_line = stored_line + ";" + annotation_line
                pathofact_attributes[protein_id] = new_line
            else:
                pathofact_attributes[protein_id] = annotation_line

    return pathofact_attributes


def parse_cdd(pathofact_attributes: dict[str, str], input_file: str) -> dict[str, str]:
    """
    Parse protein annotation file generated by local-cd-search

    Args:
        pathofact_attributes: Dictionary with protein IDs and pathofact annotations
        input_file: Path to the tsv input file
                    This file has 11 columns:
                    query hit_type pssm_id from to evalue bitscore accession short_name incomplete superfamily_pssm_id

    Returns:
        proteins_annotation defaultdict with protein id as key and cdd accession and shortname concatenated in a single string
    """
    proteins_annotation: dict[str, str] = {}
    with fileinput.hook_compressed(input_file, "r", encoding="utf-8") as input_table:
        # Skipping the header line
        next(input_table)
        for line in input_table:
            line_l: list[str] = line.rstrip().split("\t")
            protein_id: str = line_l[0]
            cdd_acc: str = line_l[7]
            cdd_short: str = line_l[8]

            if protein_id in pathofact_attributes:
                current_line = cdd_acc + ":" + cdd_short
                if protein_id in proteins_annotation:
                    stored_line = proteins_annotation[protein_id]
                    updated_line = stored_line + "," + current_line
                    proteins_annotation[protein_id] = updated_line
                else:
                    pathofact_values = pathofact_attributes[protein_id]
                    proteins_annotation[protein_id] = (
                        pathofact_values + ";cdd=" + current_line
                    )

    return proteins_annotation


def parse_ips(pathofact_attributes: dict[str, str], input_file: str) -> dict[str, str]:
    """
     Parse the Interproscan protein annotation file provided by the user

     Args:
         pathofact_attributes: Dictionary with protein IDs and pathofact annotations
         input_file: Path to the tsv input file
                     This file has 15 columns with no header:
    1. protein_id - Protein Accession (e.g. P51587)
    2. protein_md5 - Sequence MD5 digest (e.g. 14086411a2cdf1c4cba63020e1622579)
    3. protein_length - Sequence Length (e.g. 3418)
    4. analysis - Analysis (e.g. Pfam / PRINTS / Gene3D)
    5. signature_id - Signature Accession (e.g. PF09103 / G3DSA:2.40.50.140)
    6. signature_desc - Signature Description (e.g. BRCA2 repeat profile)
    7. start - Start location
    8. end - Stop location
    9. score - is the e-value (or score) of the match reported by member database method (e.g. 3.1E-52)
    10. status - is the status of the match (T: true)
    11. date - is the date of the run
    12. interpro_id - (InterPro annotations - accession (e.g. IPR002093) - optional column; only displayed if -iprlookup option is switched on)
    13. interpro_desc - (InterPro annotations - description (e.g. BRCA2 repeat) - optional column; only displayed if -iprlookup option is switched on)
    14. go_terms - (GO annotations (e.g. GO:0005515) - optional column; only displayed if –goterms option is switched on)
    15. pathway_terms - (Pathways annotations (e.g. REACT_71) - optional column; only displayed if –pathways option is switched on)

     Returns:
         proteins_annotation defaultdict with protein id as key and cdd accession and shortname concatenated in a single string
    """
    proteins_annotation: dict[str, str] = {}
    with fileinput.hook_compressed(input_file, "r", encoding="utf-8") as input_table:
        for line in input_table:
            line_l: list[str] = line.rstrip().split("\t")
            protein_id: str = line_l[0]
            analysis: str = line_l[3]
            signature_acc: str = line_l[4]
            signature_desc: str = line_l[5]

            if analysis == "CDD":
                if protein_id in pathofact_attributes:
                    current_line = signature_acc + ":" + signature_desc
                    if protein_id in proteins_annotation:
                        stored_line = proteins_annotation[protein_id]
                        updated_line = stored_line + "," + current_line
                        proteins_annotation[protein_id] = updated_line
                    else:
                        pathofact_values = pathofact_attributes[protein_id]
                        proteins_annotation[protein_id] = (
                            pathofact_values + ";cdd=" + current_line
                        )

    return proteins_annotation


def parse_gff(
    cds_gff: str, output_file: str, proteins_annotation: dict[str, str]
) -> None:
    """
    Parse GFF file and integrate Pathofact2 annotations into output GFF.

    Args:
        cds_gff: Path to input GFF file containing CDS coordinates
        output_file: Path to output GFF file
        proteins_annotation: Dictionary mapping protein IDs to attribute lists
    """
    with (
        fileinput.hook_compressed(cds_gff, "r", encoding="utf-8") as input_table,
        open(output_file, "w") as output_gff,
    ):
        output_gff.write("##gff-version 3\n")
        for line in input_table:
            line = line.rstrip()
            line_l: list[str] = line.split("\t")
            # Annotation lines have exactly 9 columns
            if len(line_l) == 9:
                (
                    contig,
                    seq_source,
                    seq_type,
                    start,
                    end,
                    score,
                    strand,
                    phase,
                    attr,
                ) = line.split("\t")
                feature_id = attr.split(";")[0].replace("ID=", "")
                if feature_id in proteins_annotation:
                    new_attribute: str = (
                        "ID=" + feature_id + ";" + proteins_annotation[feature_id]
                    )
                    line_l.pop(-1)
                    line_l.append(new_attribute)
                    to_print: str = "\t".join(line_l)
                    output_gff.write(to_print + "\n")


def main() -> None:
    """
    Main function to orchestrate Pathofact2 results
    """
    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Integration of toxins and virulence factors predicted and annotated using Pathofact2 into a single gff3 file"
    )
    parser.add_argument(
        "-g",
        "--gff",
        dest="cds_gff",
        help="GFF file containing the coordinates of the CDSs used for Pathofact2 annotation",
        required=True,
        default=None,
    )
    parser.add_argument(
        "-a",
        "--annot",
        dest="proteins_annot",
        help="Tab-delimited file of functional annotation. It can be the interproscan result for all the proteins provided by the user or the CDD annotation for relevant proteins generated as part of the pathofact subworkflow",
        required=False,
        default=None,
    )
    parser.add_argument(
        "-s",
        "--support",
        dest="pred_support",
        help="Tab-delimited file corresponding to the Pathofact2 result containing the blastp hits evalues and the probability of the predictions",
        required=False,
        default=None,
    )
    parser.add_argument(
        "-t",
        "--type",
        dest="annot_type",
        choices=["cdd", "ips"],
        help="Annotation type to be parsed. Valid strings are 'cdd' for results generated in the subworkflow by local-cd-search module or 'ips' for user provided interproscan annotation",
        required=False,
        default=None,
    )
    parser.add_argument(
        "-o",
        "--output",
        dest="output",
        help="Name of the output file",
        required=False,
        default="integrated_result.gff",
    )
    parser.add_argument(
        "-v",
        "--verbose",
        dest="verbose",
        help="Enable verbose logging (DEBUG level)",
        action="store_true",
        default=False,
    )
    args: argparse.Namespace = parser.parse_args()

    # Configure logging
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),  # Console output
        ],
    )

    valid_annot_types = ["cdd", "ips"]
    if args.annot_type not in valid_annot_types:
        logger.info(
            f"Valid annotation types are cdd or ips. {args.annot_type} is not a valid string. Exiting..."
        )
        exit()

    to_validate_inputs: dict[str, str | None] = {
        "gff": args.cds_gff,
        "annotation": args.proteins_annot,
    }

    non_valid_inputs: dict[str, str | None] = validate_inputs(to_validate_inputs)
    if len(non_valid_inputs) == 0:
        logger.info("Provided inputs are valid")
        logger.info(f"Parsing Pathofact2 predictions support file: {args.pred_support}")
        pathofact_attributes: dict[str, str] = parse_pathofact_support(
            args.pred_support
        )
        logger.debug(
            f"Pathofact2 predicted {len(pathofact_attributes)} VF and toxin proteins"
        )

        if args.annot_type == "cdd":
            logger.info(f"Parsing CDD proteins annotation file: {args.proteins_annot}")
            proteins_annotation: dict[str, str] = parse_cdd(
                pathofact_attributes, args.proteins_annot
            )

        elif args.annot_type == "ips":
            logger.info(f"Parsing IPS proteins annotation file: {args.proteins_annot}")
            proteins_annotation: dict[str, str] = parse_ips(
                pathofact_attributes, args.proteins_annot
            )

        logger.info("Parsing gff file and writing output file")
        parse_gff(args.cds_gff, args.output, proteins_annotation)
        logger.info(f"Output written to: {args.output}")

    else:
        logger.info(f"{len(non_valid_inputs)} invalid input files detected")
        for key, value in non_valid_inputs.items():
            logger.info(f"{key} file is invalid due to: {value}")
        logger.info("The output file is not generated")


if __name__ == "__main__":
    main()
